{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc0814ca-0a49-41dc-9508-03c6199bca5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "tensor([[[-0.6191, -0.6474, -0.6616,  ..., -0.3076, -0.3784, -0.4351],\n",
      "         [-0.6757, -0.6899, -0.6899,  ..., -0.3360, -0.3784, -0.4351],\n",
      "         [-0.6757, -0.6757, -0.6757,  ..., -0.3360, -0.3643, -0.3784],\n",
      "         ...,\n",
      "         [-0.6191, -0.6757, -0.7465,  ..., -0.7040, -0.7182, -0.7324],\n",
      "         [-0.6616, -0.6899, -0.7182,  ..., -0.7040, -0.7182, -0.7324],\n",
      "         [-0.7182, -0.7040, -0.6899,  ..., -0.7040, -0.7182, -0.7324]],\n",
      "\n",
      "        [[-0.7122, -0.7410, -0.7554,  ..., -0.4238, -0.4959, -0.5536],\n",
      "         [-0.7698, -0.7842, -0.7842,  ..., -0.4526, -0.4959, -0.5536],\n",
      "         [-0.7698, -0.7698, -0.7698,  ..., -0.4526, -0.4815, -0.4959],\n",
      "         ...,\n",
      "         [-0.6112, -0.6689, -0.7698,  ..., -0.7554, -0.7698, -0.7842],\n",
      "         [-0.6545, -0.6833, -0.7410,  ..., -0.7554, -0.7698, -0.7842],\n",
      "         [-0.7122, -0.6977, -0.7122,  ..., -0.7554, -0.7698, -0.7842]],\n",
      "\n",
      "        [[-0.7344, -0.7617, -0.7754,  ..., -0.4611, -0.5294, -0.5841],\n",
      "         [-0.7890, -0.8027, -0.8027,  ..., -0.4884, -0.5294, -0.5841],\n",
      "         [-0.7890, -0.7890, -0.7890,  ..., -0.4884, -0.5157, -0.5294],\n",
      "         ...,\n",
      "         [-0.5704, -0.6251, -0.7070,  ..., -0.7480, -0.7617, -0.7754],\n",
      "         [-0.6114, -0.6387, -0.6797,  ..., -0.7480, -0.7617, -0.7754],\n",
      "         [-0.6387, -0.6251, -0.6251,  ..., -0.7480, -0.7617, -0.7754]]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from torch.utils import data\n",
    "from torch import Tensor\n",
    "from typing import Tuple\n",
    "\n",
    "def crop_to_region(coords: Tuple[int], img: Tensor, crop_size: int=42) -> Tensor:\n",
    "    \"\"\" \n",
    "    Given coordinates in the form Tuple[int](y, x), return a cropped\n",
    "    sample of the input imaged centred at (y, x), matching the input size.\n",
    "    Args:\n",
    "        coords (Tuple[int]): The input coordinates (y, x) where the crop will be\n",
    "        centred.\n",
    "        img (Tensor): The input image, either 3x400x400, 3x250x250, 3x150x150\n",
    "        crop_size (int, optional): The size of the returned crop. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The image cropped with central coordinates at (y, x) of size \n",
    "        (3 x size x size) # is size here referring to 42?\n",
    "    \"\"\"\n",
    "    _, H, W = img.shape\n",
    "    y, x = coords\n",
    "    y_min, x_min = max(0, y-crop_size//2), max(0, x-crop_size//2)\n",
    "    y_max, x_max = min(H, y+crop_size//2), min(W, x+crop_size//2)\n",
    "    region = img[:, y_min:y_max, x_min:x_max]\n",
    "    if region.shape[1] < crop_size:\n",
    "        to_pad = crop_size - region.shape[1]\n",
    "        padding = (0, 0, to_pad, 0) if (y-crop_size//2) < 0 else (0, 0, 0, to_pad)\n",
    "        region = F.pad(region, padding, mode='replicate')\n",
    "\n",
    "    if region.shape[2] < crop_size:\n",
    "        to_pad = crop_size - region.shape[2]\n",
    "        padding = (to_pad, 0, 0, 0) if (x-crop_size//2) < 0 else (0, to_pad, 0, 0)\n",
    "        region = F.pad(region, padding, mode='replicate')\n",
    "    return region\n",
    "\n",
    "class MIT(data.Dataset):\n",
    "    def __init__(self, dataset_path: str):\n",
    "        \"\"\"\n",
    "        Given the dataset path, create the MIT dataset. Creates the\n",
    "        variable self.dataset which is a list of dictionaries with three keys:\n",
    "            1) X: For train the crop of image. This is of shape [3, 3, 42, 42]. The \n",
    "                first dim represents the crop across each different scale\n",
    "                (400x400, 250x250, 150x150), the second dim is the colour\n",
    "                channels C, followed by H and W (42x42). For inference, this is \n",
    "                the full size image of shape [3, H, W].\n",
    "            2) y: The label for the crop. 1 = a fixation point, 0 = a\n",
    "                non-fixation point. -1 = Unlabelled i.e. val and test\n",
    "            3) file: The file name the crops were extracted from.\n",
    "            \n",
    "        If the dataset belongs to val or test, there are 4 additional keys:\n",
    "            1) X_400: The image resized to 400x400\n",
    "            2) X_250: The image resized to 250x250\n",
    "            3) X_150: The image resized to 150x150\n",
    "            4) spatial_coords: The centre coordinates of all 50x50 (2500) crops\n",
    "            \n",
    "        These additional keys help to load the different scales within the\n",
    "        dataloader itself in a timely manner. Precomputing all crops requires too\n",
    "        much storage for the lab machines, and resizing/cropping on the fly\n",
    "        slows down the dataloader, so this is a happy balance.\n",
    "        Args:\n",
    "            dataset_path (str): Path to train/val/test.pth.tar\n",
    "        \"\"\"\n",
    "        self.dataset = torch.load(dataset_path, weights_only=True)\n",
    "        self.mode = 'train' if 'train' in dataset_path else 'inference'\n",
    "        self.num_crops = 2500 if self.mode == 'inference' else 1\n",
    "\n",
    "    def __getitem__(self, index) -> Tuple[Tensor, int]:\n",
    "        \"\"\"\n",
    "        Given the index from the DataLoader, return the image crop(s) and label\n",
    "        Args:\n",
    "            index (int): the dataset index provided by the PyTorch DataLoader.\n",
    "        Returns:\n",
    "            Tuple[Tensor, int]: A two-element tuple consisting of: \n",
    "                1) img (Tensor): The image crop of shape [3, 3, 42, 42]. The \n",
    "                first dim represents the crop across each different scale\n",
    "                (400x400, 250x250, 150x150), the second dim is the colour\n",
    "                channels C, followed by H and W (42x42).\n",
    "                2) label (int): The label for this crop. 1 = a fixation point, \n",
    "                0 = a non-fixation point. -1 = Unlabelled i.e. val and test.\n",
    "        \"\"\"\n",
    "        sample_index = index // self.num_crops\n",
    "        \n",
    "        img = self.dataset[sample_index]['X']\n",
    "        \n",
    "        # Inference crops are not precomputed due to file size, do here instead\n",
    "        if self.mode == 'inference': \n",
    "            _, H, W = img.shape\n",
    "            crop_index = index % self.num_crops\n",
    "            crop_y, crop_x = self.dataset[sample_index]['spatial_coords'][crop_index]\n",
    "            scales = []\n",
    "            for size in ['X_400', 'X_250', 'X_150']:\n",
    "                scaled_img = self.dataset[sample_index][size]\n",
    "                y_ratio, x_ratio = scaled_img.shape[1] / H, scaled_img.shape[2] / W\n",
    "                \n",
    "                # Need to rescale the crops central coordinate.\n",
    "                scaled_coords = (int(y_ratio * crop_y), int(x_ratio * crop_x))\n",
    "                crops = crop_to_region(scaled_coords, scaled_img)\n",
    "                scales.append(crops)\n",
    "            img = torch.stack(scales, axis=1)\n",
    "            \n",
    "        label = self.dataset[sample_index]['y']\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset (length of the list of dictionaries * number\n",
    "        of crops). \n",
    "        __len()__ always needs to be defined so that the DataLoader\n",
    "            can create the batches\n",
    "        Returns:\n",
    "            len(self.dataset) (int): the length of the list of dictionaries * number of\n",
    "            crops.\n",
    "        \"\"\"\n",
    "        return len(self.dataset) * self.num_crops\n",
    "\n",
    "\n",
    "trainingdata = MIT(\"data/train_data.pth.tar\")\n",
    "testingdata = MIT(\"data/test_data.pth.tar\")\n",
    "valdata = MIT(\"data/val_data.pth.tar\")\n",
    "\n",
    "\n",
    "# each element in self.dataset dictionary which has three components so just get X and y component (so X component 3x3x42x42 and y is label) -> inputs to the CNN\n",
    "\n",
    "\n",
    "print(testingdata.dataset[0]['y'])\n",
    "print(testingdata.dataset[0]['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17a9b033-2b44-4e47-9427-46952b0206b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method type of Tensor object at 0x7f9148dd8950>\n",
      "all working\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NormalisedDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx] # Stack X tensors and extract y values\n",
    "    \n",
    "X_train = torch.stack([sample['X'] for sample in trainingdata.dataset])\n",
    "y_train = torch.tensor([sample['y'] for sample in trainingdata.dataset])\n",
    "\n",
    "# Calculate mean and std for normalization\n",
    "mean_train = X_train.view(X_train.size(0), X_train.size(1), -1).mean(dim=(0, 2))\n",
    "std_train = X_train.view(X_train.size(0), X_train.size(1), -1).std(dim=(0, 2))\n",
    "\n",
    "# Normalize X_train\n",
    "normalised_X_train = (X_train - mean_train[None, :, None, None]) / std_train[None, :, None, None]\n",
    "\n",
    "# Create the custom dataset\n",
    "normalised_trainingdata = NormalisedDataset(normalised_X_train, y_train)\n",
    "\n",
    "# Pass the dataset into DataLoader\n",
    "train_loader = DataLoader(normalised_trainingdata, batch_size=128, shuffle=True)\n",
    "print('all working')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceec3f12-cda0-497d-9e52-a1e3e1695176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "training_data_augmented = trainingdata.dataset * 2\n",
    "for i in range(len(trainingdata.dataset), len(training_data_augmented)):\n",
    "    training_data_augmented[i]['X'] = torch.flip(training_data_augmented[i]['X'], dims=[3])\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a998c88-e793-41f4-a745-4a7ec688336d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019924132153391838 -0.003527475520968437 -0.011872995644807816 0.9579979777336121 0.948402464389801 0.9503791928291321\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statistics\n",
    "\n",
    "red_channel = ([sample['X'][:, 0, :, :] for sample in training_data_augmented])\n",
    "blue_channel = ([sample['X'][:, 1, :, :] for sample in training_data_augmented])\n",
    "green_channel = ([sample['X'][:, 2, :, :] for sample in training_data_augmented])\n",
    "\n",
    "normalised_training_data = training_data_augmented\n",
    "\n",
    "# red channel is a list of 3x42x42 i.e each resolution 42x42 image only the red channel, likewise with the blue and green channel\n",
    "#Â so want the mean of the red channels, blue and green channel\n",
    "\n",
    "def channel_mean(x):\n",
    "    return (torch.stack(x).mean().item())\n",
    "\n",
    "def channel_std(x):\n",
    "    return (torch.stack(x).std().item())\n",
    "\n",
    "# def channel_mean(x):\n",
    "#     channel_sum = 0 # sum is 0 to begin with\n",
    "#     for i in range(0, len(x)): # go through each item in the list\n",
    "#         item = x[i] # extract 3x42x42 list\n",
    "#         for j in range(0, 3): # find the sum of each element of the 3x42x42 list and increment to the running sum\n",
    "#             for k in range(0, 42):\n",
    "#                 for l in range(0,42):\n",
    "#                     channel_sum += item[j][k][l].item()\n",
    "                    \n",
    "#     return (channel_sum / (len(x)*3*42*42)) # mean of x i.e. total of all values / number of values \n",
    "\n",
    "red_mean = channel_mean(red_channel)\n",
    "blue_mean = channel_mean(blue_channel)\n",
    "green_mean = channel_mean(green_channel)\n",
    "\n",
    "red_std = channel_std(red_channel)\n",
    "blue_std = channel_std(blue_channel)\n",
    "green_std = channel_std(green_channel)\n",
    "\n",
    "print(red_mean, blue_mean, green_mean, red_std, blue_std, green_std)\n",
    "\n",
    "for i in range(0, len(normalised_training_data)): # goes through the current list\n",
    "    training_data_X = normalised_training_data[i]['X'] # gets the 3x3x42x42 X\n",
    "    training_data_X[:,0,:,:] = (training_data_X[:,0,:,:] - red_mean) / red_std # gets red channel values and normalises them\n",
    "    training_data_X[:, 1, :,:] = (training_data_X[:,1,:,:] - blue_mean) / blue_std\n",
    "    training_data_X[:, 2, :,:] = (training_data_X[:,2,:,:] - green_mean) / green_std\n",
    "    \n",
    "    normalised_training_data[i]['X'] = training_data_X\n",
    "\n",
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8251f61-b1e5-48c3-88f5-9aff49444a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 705, 1024])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testingdata.dataset[1]['X'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
