{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23799f4b-7e50-4a82-8c46-54e56affb614",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eefa5dd-a923-4451-9e6b-d90487a2a6f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import time\n",
    "from multiprocessing import cpu_count\n",
    "from typing import Union, NamedTuple\n",
    "\n",
    "from torch.utils import data\n",
    "from torch import Tensor\n",
    "from typing import Tuple\n",
    "import torch\n",
    "import torch.backends.cudnn\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import torchvision.datasets\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def crop_to_region(coords: Tuple[int], img: Tensor, crop_size: int=42) -> Tensor:\n",
    "    \"\"\" \n",
    "    Given coordinates in the form Tuple[int](y, x), return a cropped\n",
    "    sample of the input imaged centred at (y, x), matching the input size.\n",
    "    Args:\n",
    "        coords (Tuple[int]): The input coordinates (y, x) where the crop will be\n",
    "        centred.\n",
    "        img (Tensor): The input image, either 3x400x400, 3x250x250, 3x150x150\n",
    "        crop_size (int, optional): The size of the returned crop. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The image cropped with central coordinates at (y, x) of size \n",
    "        (3 x size x size) # is size here referring to 42?\n",
    "    \"\"\"\n",
    "    _, H, W = img.shape\n",
    "    y, x = coords\n",
    "    y_min, x_min = max(0, y-crop_size//2), max(0, x-crop_size//2)\n",
    "    y_max, x_max = min(H, y+crop_size//2), min(W, x+crop_size//2)\n",
    "    region = img[:, y_min:y_max, x_min:x_max]\n",
    "    if region.shape[1] < crop_size:\n",
    "        to_pad = crop_size - region.shape[1]\n",
    "        padding = (0, 0, to_pad, 0) if (y-crop_size//2) < 0 else (0, 0, 0, to_pad)\n",
    "        region = F.pad(region, padding, mode='replicate')\n",
    "\n",
    "    if region.shape[2] < crop_size:\n",
    "        to_pad = crop_size - region.shape[2]\n",
    "        padding = (to_pad, 0, 0, 0) if (x-crop_size//2) < 0 else (0, to_pad, 0, 0)\n",
    "        region = F.pad(region, padding, mode='replicate')\n",
    "    return region\n",
    "\n",
    "class MIT(data.Dataset):\n",
    "    def __init__(self, dataset_path: str):\n",
    "        \"\"\"\n",
    "        Given the dataset path, create the MIT dataset. Creates the\n",
    "        variable self.dataset which is a list of dictionaries with three keys:\n",
    "            1) X: For train the crop of image. This is of shape [3, 3, 42, 42]. The \n",
    "                first dim represents the crop across each different scale\n",
    "                (400x400, 250x250, 150x150), the second dim is the colour\n",
    "                channels C, followed by H and W (42x42). For inference, this is \n",
    "                the full size image of shape [3, H, W].\n",
    "            2) y: The label for the crop. 1 = a fixation point, 0 = a\n",
    "                non-fixation point. -1 = Unlabelled i.e. val and test\n",
    "            3) file: The file name the crops were extracted from.\n",
    "            \n",
    "        If the dataset belongs to val or test, there are 4 additional keys:\n",
    "            1) X_400: The image resized to 400x400\n",
    "            2) X_250: The image resized to 250x250\n",
    "            3) X_150: The image resized to 150x150\n",
    "            4) spatial_coords: The centre coordinates of all 50x50 (2500) crops\n",
    "            \n",
    "        These additional keys help to load the different scales within the\n",
    "        dataloader itself in a timely manner. Precomputing all crops requires too\n",
    "        much storage for the lab machines, and resizing/cropping on the fly\n",
    "        slows down the dataloader, so this is a happy balance.\n",
    "        Args:\n",
    "            dataset_path (str): Path to train/val/test.pth.tar\n",
    "        \"\"\"\n",
    "        self.dataset = torch.load(dataset_path, weights_only=True)\n",
    "        self.mode = 'train' if 'train' in dataset_path else 'inference'\n",
    "        self.num_crops = 2500 if self.mode == 'inference' else 1\n",
    "\n",
    "    def __getitem__(self, index) -> Tuple[Tensor, int]:\n",
    "        \"\"\"\n",
    "        Given the index from the DataLoader, return the image crop(s) and label\n",
    "        Args:\n",
    "            index (int): the dataset index provided by the PyTorch DataLoader.\n",
    "        Returns:\n",
    "            Tuple[Tensor, int]: A two-element tuple consisting of: \n",
    "                1) img (Tensor): The image crop of shape [3, 3, 42, 42]. The \n",
    "                first dim represents the crop across each different scale\n",
    "                (400x400, 250x250, 150x150), the second dim is the colour\n",
    "                channels C, followed by H and W (42x42).\n",
    "                2) label (int): The label for this crop. 1 = a fixation point, \n",
    "                0 = a non-fixation point. -1 = Unlabelled i.e. val and test.\n",
    "        \"\"\"\n",
    "        sample_index = index // self.num_crops\n",
    "        \n",
    "        img = self.dataset[sample_index]['X']\n",
    "        \n",
    "        # Inference crops are not precomputed due to file size, do here instead\n",
    "        if self.mode == 'inference': \n",
    "            _, H, W = img.shape\n",
    "            crop_index = index % self.num_crops\n",
    "            crop_y, crop_x = self.dataset[sample_index]['spatial_coords'][crop_index]\n",
    "            scales = []\n",
    "            for size in ['X_400', 'X_250', 'X_150']:\n",
    "                scaled_img = self.dataset[sample_index][size]\n",
    "                y_ratio, x_ratio = scaled_img.shape[1] / H, scaled_img.shape[2] / W\n",
    "                \n",
    "                # Need to rescale the crops central coordinate.\n",
    "                scaled_coords = (int(y_ratio * crop_y), int(x_ratio * crop_x))\n",
    "                crops = crop_to_region(scaled_coords, scaled_img)\n",
    "                scales.append(crops)\n",
    "            img = torch.stack(scales, axis=1)\n",
    "            \n",
    "        label = self.dataset[sample_index]['y']\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset (length of the list of dictionaries * number\n",
    "        of crops). \n",
    "        __len()__ always needs to be defined so that the DataLoader\n",
    "            can create the batches\n",
    "        Returns:\n",
    "            len(self.dataset) (int): the length of the list of dictionaries * number of\n",
    "            crops.\n",
    "        \"\"\"\n",
    "        return len(self.dataset) * self.num_crops\n",
    "\n",
    "\n",
    "trainingdata = MIT(\"data/train_data.pth.tar\")\n",
    "testingdata = MIT(\"data/test_data.pth.tar\")\n",
    "\n",
    "\n",
    "# each element in self.dataset dictionary which has three components so just get X and y component (so X component 3x3x42x42 and y is label) -> inputs to the CNN\n",
    "\n",
    "\n",
    "# print(trainingdata.dataset[0]['y'])\n",
    "# print(trainingdata.dataset[0]['X'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7466725c-8aa7-4d9f-9e54-607438324408",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lab 4 code pasted below as general starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b98698d5-be75-4d86-842e-9916707cba5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24090, 3, 3, 42, 42])\n",
      "Namespace(dataset_root=PosixPath('/home/tm21064/.cache/torch/datasets'), log_dir=PosixPath('logs'), learning_rate=0.0002, batch_size=128, epochs=20, val_frequency=2, log_frequency=10, print_frequency=10, worker_count=0, sgd_momentum=0, data_aug_hflip=False)\n",
      "Writing logs to logs/CNN_bn_bs=128_lr=0.0002_momentum=0.9_run_47\n",
      "About to train...\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5390625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5078125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5234375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5234375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.515625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.515625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.59375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.6015625\n",
      "epoch: [0], step: [9/189], batch loss: 0.80901, batch accuracy: 60.16, data load time: 0.00174, step time: 0.04481\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5390625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.4921875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5859375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.4921875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.53125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5078125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5078125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.609375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5390625\n",
      "epoch: [0], step: [19/189], batch loss: 0.90236, batch accuracy: 53.91, data load time: 0.00179, step time: 0.04477\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.4921875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.4765625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.515625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.59375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.484375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.6015625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.4921875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.53125\n",
      "epoch: [0], step: [29/189], batch loss: 0.82973, batch accuracy: 53.12, data load time: 0.00192, step time: 0.04534\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.4921875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5859375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.53125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5234375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.59375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.609375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.59375\n",
      "epoch: [0], step: [39/189], batch loss: 0.84352, batch accuracy: 59.38, data load time: 0.00184, step time: 0.04515\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.4765625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5390625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.53125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5234375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5703125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.390625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5703125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5078125\n",
      "epoch: [0], step: [49/189], batch loss: 0.87383, batch accuracy: 50.78, data load time: 0.00177, step time: 0.04514\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5078125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.6171875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5234375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.4765625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.484375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.6171875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.546875\n",
      "epoch: [0], step: [59/189], batch loss: 0.81970, batch accuracy: 54.69, data load time: 0.00178, step time: 0.04496\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.53125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.515625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.4453125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.484375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5703125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.515625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5390625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.59375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.53125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.4921875\n",
      "epoch: [0], step: [69/189], batch loss: 0.88933, batch accuracy: 49.22, data load time: 0.00172, step time: 0.04499\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.4765625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5703125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.515625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.4765625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5703125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5234375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.515625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.609375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5390625\n",
      "epoch: [0], step: [79/189], batch loss: 0.88354, batch accuracy: 53.91, data load time: 0.00198, step time: 0.04855\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.515625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.609375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5859375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5859375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.578125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.6015625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.59375\n",
      "epoch: [0], step: [89/189], batch loss: 0.76699, batch accuracy: 59.38, data load time: 0.00191, step time: 0.04888\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.4609375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.515625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5234375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5390625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.578125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5859375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5703125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.546875\n",
      "epoch: [0], step: [99/189], batch loss: 0.83259, batch accuracy: 54.69, data load time: 0.00193, step time: 0.04504\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5390625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5703125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5390625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5390625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5390625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5546875\n",
      "epoch: [0], step: [109/189], batch loss: 0.83770, batch accuracy: 55.47, data load time: 0.00190, step time: 0.05007\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5078125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5390625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.4921875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.6328125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.53125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.4921875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.46875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5078125\n",
      "epoch: [0], step: [119/189], batch loss: 0.86308, batch accuracy: 50.78, data load time: 0.00182, step time: 0.04502\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5078125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.515625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5390625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.515625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.53125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5078125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.4765625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.53125\n",
      "epoch: [0], step: [129/189], batch loss: 0.86433, batch accuracy: 53.12, data load time: 0.00194, step time: 0.04511\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.453125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5234375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5703125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.4609375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5390625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.46875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.515625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.484375\n",
      "epoch: [0], step: [139/189], batch loss: 0.86852, batch accuracy: 48.44, data load time: 0.00188, step time: 0.04529\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5234375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5390625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5078125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.53125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.59375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5703125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5234375\n",
      "epoch: [0], step: [149/189], batch loss: 0.86850, batch accuracy: 52.34, data load time: 0.00195, step time: 0.04680\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.53125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5234375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.53125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.515625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.609375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.53125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5703125\n",
      "epoch: [0], step: [159/189], batch loss: 0.84240, batch accuracy: 57.03, data load time: 0.00184, step time: 0.04511\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.59375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.4765625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.640625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.4765625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5703125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5859375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5703125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.578125\n",
      "epoch: [0], step: [169/189], batch loss: 0.77401, batch accuracy: 57.81, data load time: 0.00188, step time: 0.04692\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.578125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5078125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.6171875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.53125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.609375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.484375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5859375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5078125\n",
      "epoch: [0], step: [179/189], batch loss: 0.83702, batch accuracy: 50.78, data load time: 0.00228, step time: 0.04764\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.4609375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5390625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5703125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5234375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.515625\n",
      "fc1 layer shape:  torch.Size([26, 512])\n",
      "0.5\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5546875\n",
      "epoch: [1], step: [0/189], batch loss: 0.83191, batch accuracy: 55.47, data load time: 0.00306, step time: 0.04387\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.453125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.59375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.578125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.53125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5390625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.53125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.53125\n",
      "epoch: [1], step: [10/189], batch loss: 0.87175, batch accuracy: 53.12, data load time: 0.00195, step time: 0.04786\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5703125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.4609375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.53125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.484375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.484375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5234375\n",
      "epoch: [1], step: [20/189], batch loss: 0.83698, batch accuracy: 52.34, data load time: 0.00184, step time: 0.04675\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5234375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5703125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5234375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.515625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.59375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.59375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5625\n",
      "epoch: [1], step: [30/189], batch loss: 0.84258, batch accuracy: 56.25, data load time: 0.00190, step time: 0.04888\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5234375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.453125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.4765625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.53125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5703125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.46875\n",
      "epoch: [1], step: [40/189], batch loss: 0.87626, batch accuracy: 46.88, data load time: 0.00180, step time: 0.04683\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.515625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5703125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5078125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.59375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5234375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.46875\n",
      "epoch: [1], step: [50/189], batch loss: 0.85675, batch accuracy: 46.88, data load time: 0.00190, step time: 0.04528\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5234375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.515625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.53125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5390625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5859375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.53125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.6484375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5234375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.515625\n",
      "epoch: [1], step: [60/189], batch loss: 0.87767, batch accuracy: 51.56, data load time: 0.00206, step time: 0.04513\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.59375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.578125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.515625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.59375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.53125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5625\n",
      "epoch: [1], step: [70/189], batch loss: 0.81231, batch accuracy: 56.25, data load time: 0.00182, step time: 0.04748\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.484375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.453125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5703125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.6015625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.515625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.53125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5234375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.53125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5703125\n",
      "epoch: [1], step: [80/189], batch loss: 0.85807, batch accuracy: 57.03, data load time: 0.00186, step time: 0.04534\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5390625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.4921875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5859375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5703125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5234375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.515625\n",
      "epoch: [1], step: [90/189], batch loss: 0.90783, batch accuracy: 51.56, data load time: 0.00182, step time: 0.04536\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.578125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.6015625\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5859375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.46875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.46875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5234375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5546875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5390625\n",
      "epoch: [1], step: [100/189], batch loss: 0.83241, batch accuracy: 53.91, data load time: 0.00186, step time: 0.04551\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.4921875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.609375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.46875\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5859375\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5703125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5703125\n",
      "fc1 layer shape:  torch.Size([128, 512])\n",
      "0.5859375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 520\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    519\u001b[0m     args, _ \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_known_args()\n\u001b[0;32m--> 520\u001b[0m     main(args)\n",
      "Cell \u001b[0;32mIn[5], line 129\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    124\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    125\u001b[0m     model, train_loader, test_loader, criterion, optimizer, summary_writer, DEVICE\n\u001b[1;32m    126\u001b[0m )\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAbout to train...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 129\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[1;32m    130\u001b[0m     args\u001b[38;5;241m.\u001b[39mepochs,\n\u001b[1;32m    131\u001b[0m     args\u001b[38;5;241m.\u001b[39mval_frequency,\n\u001b[1;32m    132\u001b[0m     print_frequency\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mprint_frequency,\n\u001b[1;32m    133\u001b[0m     log_frequency\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlog_frequency,\n\u001b[1;32m    134\u001b[0m )\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinsished training.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    136\u001b[0m summary_writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn[5], line 365\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, epochs, val_frequency, print_frequency, log_frequency, start_epoch)\u001b[0m\n\u001b[1;32m    360\u001b[0m data_load_end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m## TASK 1: Compute the forward pass of the model, print the output shape\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m##         and quit the program\u001b[39;00m\n\u001b[0;32m--> 365\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(batch)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    366\u001b[0m \u001b[38;5;66;03m#print(logits.shape)\u001b[39;00m\n\u001b[1;32m    367\u001b[0m                         \n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m#import sys; sys.exit(1)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m## TASK 9: Compute the loss using self.criterion and\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m##         store it in a variable called `loss`\u001b[39;00m\n\u001b[1;32m    378\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(logits, labels\u001b[38;5;241m.\u001b[39mfloat())\n",
      "Cell \u001b[0;32mIn[5], line 254\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m    251\u001b[0m images2 \u001b[38;5;241m=\u001b[39m images[:, \u001b[38;5;241m1\u001b[39m, :, :, :]\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m#pass second resolutions through the network\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m x2 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(images2))\n\u001b[1;32m    255\u001b[0m x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool1(x2)\n\u001b[1;32m    256\u001b[0m x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatchnorm1(x2)\n",
      "File \u001b[0;32m/opt/anaconda/3-2024/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda/3-2024/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda/3-2024/lib/python3.12/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m/opt/anaconda/3-2024/lib/python3.12/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "    description=\"Train a simple CNN on CIFAR-10\",\n",
    "    formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n",
    ")\n",
    "default_dataset_dir = Path.home() / \".cache\" / \"torch\" / \"datasets\"\n",
    "parser.add_argument(\"--dataset-root\", default=default_dataset_dir)\n",
    "parser.add_argument(\"--log-dir\", default=Path(\"logs\"), type=Path)\n",
    "parser.add_argument(\"--learning-rate\", default=2e-4, type=float, help=\"Learning rate\")\n",
    "parser.add_argument(\n",
    "    \"--batch-size\",\n",
    "    default=128,\n",
    "    type=int,\n",
    "    help=\"Number of images within each mini-batch\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--epochs\",\n",
    "    default=20,\n",
    "    type=int,\n",
    "    help=\"Number of epochs (passes through the entire dataset) to train for\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--val-frequency\",\n",
    "    default=2,\n",
    "    type=int,\n",
    "    help=\"How frequently to test the model on the validation set in number of epochs\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--log-frequency\",\n",
    "    default=10,\n",
    "    type=int,\n",
    "    help=\"How frequently to save logs to tensorboard in number of steps\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--print-frequency\",\n",
    "    default=10,\n",
    "    type=int,\n",
    "    help=\"How frequently to print progress to the command line in number of steps\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-j\",\n",
    "    \"--worker-count\",\n",
    "    default=0, #cpu_count(), #note make this 0 if not using lab machine\n",
    "    type=int,\n",
    "    help=\"Number of worker processes used to load data.\",\n",
    ")\n",
    "parser.add_argument(\"--sgd-momentum\", default=0, type=float)\n",
    "parser.add_argument(\"--data-aug-hflip\", action=\"store_true\")\n",
    "\n",
    "\n",
    "\n",
    "class ImageShape(NamedTuple):\n",
    "    height: int\n",
    "    width: int\n",
    "    channels: int\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    \n",
    "\n",
    "    X_train = torch.stack([sample['X'] for sample in trainingdata.dataset]) # stack X tensors to get tensor of shape num_samples, 3, 3, 42, 42 \n",
    "\n",
    "    mean_train = X_train.view(X_train.size(0), X_train.size(1), -1).mean(dim=(0,2)) # mean and std across all samples for each channel reshaped to samples, channels=3, height*width \n",
    "    std_train = X_train.view(X_train.size(0), X_train.size(1), -1).std(dim=(0,2))\n",
    "\n",
    "    normalised_X_train = (X_train - mean_train[None, :, None, None]) / std_train[None, :, None, None]\n",
    "\n",
    "    print(normalised_X_train.shape)\n",
    "\n",
    "    normalised_trainingdata = []\n",
    "    for i in range(len(trainingdata.dataset)):\n",
    "        normalised_sample = {'X':normalised_X_train[i], 'y':trainingdata.dataset[i]['y']}\n",
    "        normalised_trainingdata.append(normalised_sample)\n",
    "    \n",
    "    normalised_trainingdata[0]['y']\n",
    "    normalised_trainingdata[0]['X']\n",
    "    print(args)\n",
    "    \n",
    "    args.dataset_root.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "\n",
    "    if False:\n",
    "        train_dataset = None #torchvision.datasets.CIFAR10(args.dataset_root, train=True, download=True, transform=transforms.Compose([transforms.RandomHorizontalFlip(), transforms.ToTensor()]))\n",
    "    else:\n",
    "        train_dataset = trainingdata #normalised_trainingdata # torchvision.datasets.CIFAR10(args.dataset_root, train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "    test_dataset = testingdata.dataset #torchvision.datasets.CIFAR10(args.dataset_root, train=False, download=False, transform=transforms.ToTensor())\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,\n",
    "        batch_size=args.batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=args.worker_count,\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.worker_count,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    model = CNN(height=42, width=42, channels=3, class_count=2)\n",
    "\n",
    "    ## TASK 8: Redefine the criterion to be softmax cross entropy\n",
    "    #criterion = nn.CrossEntropyLoss()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    ## TASK 11: Define the optimizer\n",
    "    optimizer = None #torch.optim.SGD(model.parameters(), lr=args.learning_rate, momentum=args.sgd_momentum)\n",
    "\n",
    "    log_dir = get_summary_writer_log_dir(args)\n",
    "    print(f\"Writing logs to {log_dir}\")\n",
    "    summary_writer = SummaryWriter(\n",
    "            str(log_dir),\n",
    "            flush_secs=5\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model, train_loader, test_loader, criterion, optimizer, summary_writer, DEVICE\n",
    "    )\n",
    "\n",
    "    print(\"About to train...\")\n",
    "    trainer.train(\n",
    "        args.epochs,\n",
    "        args.val_frequency,\n",
    "        print_frequency=args.print_frequency,\n",
    "        log_frequency=args.log_frequency,\n",
    "    )\n",
    "    print(\"Finsished training.\")\n",
    "    summary_writer.close()\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, height: int, width: int, channels: int, class_count: int):\n",
    "        super().__init__()\n",
    "        self.input_shape = ImageShape(height=height, width=width, channels=channels)\n",
    "        self.class_count = class_count\n",
    "\n",
    "        # Define 1st Conv, Pool, and BatchNorm\n",
    "        \n",
    "        # Conv 1\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=self.input_shape.channels,\n",
    "            out_channels=96,\n",
    "            kernel_size=(7, 7),\n",
    "            padding= (0,0), #(2, 2),\n",
    "        )\n",
    "        self.initialise_layer(self.conv1)\n",
    "        \n",
    "        # Pool 1\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "        # BatchNorm 1\n",
    "        self.batchnorm1 = nn.BatchNorm2d(self.conv1.out_channels)\n",
    "\n",
    "        # Define 2nd Conv, Pool, and BatchNorm\n",
    "        \n",
    "        # Conv 2\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=96,\n",
    "            out_channels=160,\n",
    "            kernel_size=(3, 3),\n",
    "            padding= (0,0) #(2, 2)\n",
    "        )\n",
    "\n",
    "        self.initialise_layer(self.conv2)\n",
    "        \n",
    "        # Pool 2\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "        # BatchNorm 2\n",
    "        self.batchnorm2 = nn.BatchNorm2d(num_features=self.conv2.out_channels)\n",
    "        \n",
    "        # Define 3rd Conv, Pool, and BatchNorm\n",
    "        \n",
    "        # Conv 3\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=160,\n",
    "            out_channels=288,\n",
    "            kernel_size=(3, 3),\n",
    "            padding= (0,0) #(2, 2)\n",
    "        )\n",
    "\n",
    "        self.initialise_layer(self.conv3)\n",
    "        \n",
    "        # Pool 3\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "        # BatchNorm 3\n",
    "        self.batchnorm3 = nn.BatchNorm2d(num_features=self.conv3.out_channels)\n",
    "\n",
    "        \n",
    "        # Define FC 1\n",
    "\n",
    "        self.fc1 = nn.Linear(2592, 512)\n",
    "        self.initialise_layer(self.fc1)\n",
    "\n",
    "        # BatchNorm layer after first FC layer\n",
    "        # import pdb; pdb.set_trace()\n",
    "        self.batchnorm4 = nn.BatchNorm1d(num_features=self.fc1.out_features)\n",
    "\n",
    "        \n",
    "        # Define FC 2\n",
    "\n",
    "        self.fc2 = nn.Linear(1536, 512)\n",
    "        self.initialise_layer(self.fc2)\n",
    "\n",
    "        # BatchNorm layer after second FC layer\n",
    "\n",
    "        self.batchnorm5 = nn.BatchNorm1d(num_features=self.fc2.out_features)\n",
    "        \n",
    "        # Define FC 3\n",
    "\n",
    "        self.fc3 = nn.Linear(512, 1)\n",
    "        self.initialise_layer(self.fc3)\n",
    "\n",
    "        # BatchNorm layer after third FC layer\n",
    "        self.batchnorm6 = nn.BatchNorm1d(num_features=self.fc3.out_features)\n",
    "    \n",
    "    # FORWARD METHOD for running 3 parallel CNNs\n",
    "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        #images = images.reshape(-1, 3, 42, 42)\n",
    "        images1 = images[:, 0, :, :, :]\n",
    "        \n",
    "        #pass first resolutions through the network\n",
    "        x1 = F.relu(self.conv1(images1))\n",
    "        x1 = self.pool1(x1)\n",
    "        x1 = self.batchnorm1(x1)\n",
    "\n",
    "        x1 = F.relu(self.conv2(x1))\n",
    "        x1 = self.pool2(x1)\n",
    "        x1 = self.batchnorm2(x1)\n",
    "        \n",
    "        x1 = F.relu(self.conv3(x1))\n",
    "        x1 = self.pool3(x1)\n",
    "        x1 = self.batchnorm3(x1)\n",
    "\n",
    "        # Flatten the output of the pooling layer so it is of shape (batch_size, 4096)\n",
    "        x1 = torch.flatten(x1, start_dim=1)\n",
    "        \n",
    "        # Pass x through the first fully connected layer\n",
    "        x1 = F.relu(self.fc1(x1))\n",
    "        x1 = self.batchnorm4(x1)\n",
    "        \n",
    "        images2 = images[:, 1, :, :, :]\n",
    "        \n",
    "        #pass second resolutions through the network\n",
    "        x2 = F.relu(self.conv1(images2))\n",
    "        x2 = self.pool1(x2)\n",
    "        x2 = self.batchnorm1(x2)\n",
    "\n",
    "        x2 = F.relu(self.conv2(x2))\n",
    "        x2 = self.pool2(x2)\n",
    "        x2 = self.batchnorm2(x2)\n",
    "        \n",
    "        x2 = F.relu(self.conv3(x2))\n",
    "        x2 = self.pool3(x2)\n",
    "        x2 = self.batchnorm3(x2)\n",
    "\n",
    "        # Flatten the output of the pooling layer so it is of shape (batch_size, 4096)\n",
    "        x2 = torch.flatten(x2, start_dim=1)\n",
    "        \n",
    "        # Pass x through the first fully connected layer\n",
    "        x2 = F.relu(self.fc1(x2))\n",
    "        x2 = self.batchnorm4(x2)\n",
    "        \n",
    "        \n",
    "        images3 = images[:, 2, :, :, :]\n",
    "        \n",
    "        #pass third resolutions through the network\n",
    "        x3 = F.relu(self.conv1(images3))\n",
    "        x3 = self.pool1(x3)\n",
    "        x3 = self.batchnorm1(x3)\n",
    "\n",
    "        x3 = F.relu(self.conv2(x3))\n",
    "        x3 = self.pool2(x3)\n",
    "        x3 = self.batchnorm2(x3)\n",
    "        \n",
    "        x3 = F.relu(self.conv3(x3))\n",
    "        x3 = self.pool3(x3)\n",
    "        x3 = self.batchnorm3(x3)\n",
    "\n",
    "        # Flatten the output of the pooling layer so it is of shape (batch_size, 4096)\n",
    "        x3 = torch.flatten(x3, start_dim=1)\n",
    "        \n",
    "        # Pass x through the first fully connected layer\n",
    "        x3 = F.relu(self.fc1(x3))\n",
    "        x3 = self.batchnorm4(x3)\n",
    "        \n",
    "        \n",
    "        ## TASK 6-2: Pass x through the last fully connected layer\n",
    "        #IMPORTANT: concatenate fc layers before next steps\n",
    "        xCat = torch.cat((x1, x2, x3), dim=1)\n",
    "        x = self.fc2(xCat)\n",
    "        x = self.batchnorm5(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.batchnorm6(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def initialise_layer(layer):\n",
    "        if hasattr(layer, \"bias\"):\n",
    "            nn.init.zeros_(layer.bias)\n",
    "        if hasattr(layer, \"weight\"):\n",
    "            nn.init.kaiming_normal_(layer.weight)\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        criterion: nn.Module,\n",
    "        optimizer: Optimizer,\n",
    "        summary_writer: SummaryWriter,\n",
    "        device: torch.device,\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.summary_writer = summary_writer\n",
    "        self.step = 0\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        epochs: int,\n",
    "        val_frequency: int,\n",
    "        print_frequency: int = 20,\n",
    "        log_frequency: int = 5,\n",
    "        start_epoch: int = 0\n",
    "    ):\n",
    "        self.model.train()\n",
    "        \n",
    "\n",
    "        \n",
    "        #import sys; sys.exit(1)\n",
    "        for epoch in range(start_epoch, epochs):\n",
    "            self.model.train()\n",
    "            \n",
    "            data_load_start_time = time.time()\n",
    "             #stuck after this\n",
    "            for batch, labels in self.train_loader:\n",
    "                \n",
    "                batch = batch.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                data_load_end_time = time.time()\n",
    "\n",
    "\n",
    "                ## TASK 1: Compute the forward pass of the model, print the output shape\n",
    "                ##         and quit the program\n",
    "                logits = self.model.forward(batch).squeeze()\n",
    "                #print(logits.shape)\n",
    "                                        \n",
    "                #import sys; sys.exit(1)\n",
    "\n",
    "                ## TASK 7: Rename `output` to `logits`, remove the output shape printing\n",
    "                ##         and get rid of the `import sys; sys.exit(1)`\n",
    "\n",
    "                # Task 7 done above\n",
    "\n",
    "                ## TASK 9: Compute the loss using self.criterion and\n",
    "                ##         store it in a variable called `loss`\n",
    "\n",
    "                loss = self.criterion(logits, labels.float())\n",
    "\n",
    "                ## TASK 10: Compute the backward pass\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                ## TASK 12: Step the optimizer and then zero out the gradient buffers.\n",
    "                \n",
    "                #self.optimizer.step()\n",
    "\n",
    "                #self.optimizer.zero_grad()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    preds = (logits > 0.5).float()\n",
    "                    accuracy = compute_accuracy(labels, preds)\n",
    "                    \n",
    "                data_load_time = data_load_end_time - data_load_start_time\n",
    "                step_time = time.time() - data_load_end_time\n",
    "                if ((self.step + 1) % log_frequency) == 0:\n",
    "                    self.log_metrics(epoch, accuracy, loss, data_load_time, step_time)\n",
    "                if ((self.step + 1) % print_frequency) == 0:\n",
    "                    self.print_metrics(epoch, accuracy, loss, data_load_time, step_time)\n",
    "\n",
    "                self.step += 1\n",
    "                data_load_start_time = time.time()\n",
    "\n",
    "            self.summary_writer.add_scalar(\"epoch\", epoch, self.step)\n",
    "            if ((epoch + 1) % val_frequency) == 0:\n",
    "                self.validate()\n",
    "                # self.validate() will put the model in validation mode,\n",
    "                # so we have to switch back to train mode afterwards\n",
    "                self.model.train()\n",
    "\n",
    "    def print_metrics(self, epoch, accuracy, loss, data_load_time, step_time):\n",
    "        epoch_step = self.step % len(self.train_loader)\n",
    "        print(\n",
    "                f\"epoch: [{epoch}], \"\n",
    "                f\"step: [{epoch_step}/{len(self.train_loader)}], \"\n",
    "                f\"batch loss: {loss:.5f}, \"\n",
    "                f\"batch accuracy: {accuracy * 100:2.2f}, \"\n",
    "                f\"data load time: \"\n",
    "                f\"{data_load_time:.5f}, \"\n",
    "                f\"step time: {step_time:.5f}\"\n",
    "        )\n",
    "\n",
    "    def log_metrics(self, epoch, accuracy, loss, data_load_time, step_time):\n",
    "        self.summary_writer.add_scalar(\"epoch\", epoch, self.step)\n",
    "        self.summary_writer.add_scalars(\n",
    "                \"accuracy\",\n",
    "                {\"train\": accuracy},\n",
    "                self.step\n",
    "        )\n",
    "        self.summary_writer.add_scalars(\n",
    "                \"loss\",\n",
    "                {\"train\": float(loss.item())},\n",
    "                self.step\n",
    "        )\n",
    "        self.summary_writer.add_scalar(\n",
    "                \"time/data\", data_load_time, self.step\n",
    "        )\n",
    "        self.summary_writer.add_scalar(\n",
    "                \"time/data\", step_time, self.step\n",
    "        )\n",
    "\n",
    "    def validate(self):\n",
    "        results = {\"preds\": [], \"labels\": []}\n",
    "        total_loss = 0\n",
    "        self.model.eval()\n",
    "\n",
    "        # No need to track gradients for validation, we're not optimizing.\n",
    "        with torch.no_grad():\n",
    "            for batch, labels in self.val_loader:\n",
    "                batch = batch.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                logits = self.model(batch)\n",
    "                loss = self.criterion(logits, labels)\n",
    "                total_loss += loss.item()\n",
    "                preds = logits.argmax(dim=-1).cpu().numpy()\n",
    "                results[\"preds\"].extend(list(preds))\n",
    "                results[\"labels\"].extend(list(labels.cpu().numpy()))\n",
    "\n",
    "        accuracy = compute_accuracy(\n",
    "            np.array(results[\"labels\"]), np.array(results[\"preds\"])\n",
    "        )\n",
    "        average_loss = total_loss / len(self.val_loader)\n",
    "\n",
    "        self.summary_writer.add_scalars(\n",
    "                \"accuracy\",\n",
    "                {\"test\": accuracy},\n",
    "                self.step\n",
    "        )\n",
    "        self.summary_writer.add_scalars(\n",
    "                \"loss\",\n",
    "                {\"test\": average_loss},\n",
    "                self.step\n",
    "        )\n",
    "        print(f\"validation loss: {average_loss:.5f}, accuracy: {accuracy * 100:2.2f}\")\n",
    "\n",
    "\n",
    "def compute_accuracy(\n",
    "    labels: Union[torch.Tensor, np.ndarray], preds: Union[torch.Tensor, np.ndarray]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        labels: ``(batch_size, class_count)`` tensor or array containing example labels\n",
    "        preds: ``(batch_size, class_count)`` tensor or array containing model prediction\n",
    "    \"\"\"\n",
    "    assert len(labels) == len(preds)\n",
    "    return float((labels == preds).sum()) / len(labels)\n",
    "\n",
    "\n",
    "def get_summary_writer_log_dir(args: argparse.Namespace) -> str:\n",
    "    \"\"\"Get a unique directory that hasn't been logged to before for use with a TB\n",
    "    SummaryWriter.\n",
    "\n",
    "    Args:\n",
    "        args: CLI Arguments\n",
    "\n",
    "    Returns:\n",
    "        Subdirectory of log_dir with unique subdirectory name to prevent multiple runs\n",
    "        from getting logged to the same TB log directory (which you can't easily\n",
    "        untangle in TB).\n",
    "    \"\"\"\n",
    "    tb_log_dir_prefix = (\n",
    "      f\"CNN_bn_\"\n",
    "      f\"bs={args.batch_size}_\"\n",
    "      f\"lr={args.learning_rate}_\"\n",
    "      f\"momentum=0.9_\" +\n",
    "      (\"hflip_\" if args.data_aug_hflip else \"\") +\n",
    "      f\"run_\"\n",
    "  )\n",
    "    i = 0\n",
    "    while i < 1000:\n",
    "        tb_log_dir = args.log_dir / (tb_log_dir_prefix + str(i))\n",
    "        if not tb_log_dir.exists():\n",
    "            return str(tb_log_dir)\n",
    "        i += 1\n",
    "    return str(tb_log_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args, _ = parser.parse_known_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829a902b-94c1-46ef-855b-e48ceacd2a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
