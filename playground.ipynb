{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7466725c-8aa7-4d9f-9e54-607438324408",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lab 4 code pasted below as general starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b98698d5-be75-4d86-842e-9916707cba5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--dataset-root DATASET_ROOT]\n",
      "                             [--log-dir LOG_DIR]\n",
      "                             [--learning-rate LEARNING_RATE]\n",
      "                             [--batch-size BATCH_SIZE] [--epochs EPOCHS]\n",
      "                             [--val-frequency VAL_FREQUENCY]\n",
      "                             [--log-frequency LOG_FREQUENCY]\n",
      "                             [--print-frequency PRINT_FREQUENCY]\n",
      "                             [-j WORKER_COUNT] [--sgd-momentum SGD_MOMENTUM]\n",
      "                             [--data-aug-hflip]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/qz21635/.local/share/jupyter/runtime/kernel-a35d714e-b969-497c-a059-a641a6a681e8.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/3-2024/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import time\n",
    "from multiprocessing import cpu_count\n",
    "from typing import Union, NamedTuple\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import torchvision.datasets\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "    description=\"Train a simple CNN on CIFAR-10\",\n",
    "    formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n",
    ")\n",
    "default_dataset_dir = Path.home() / \".cache\" / \"torch\" / \"datasets\"\n",
    "parser.add_argument(\"--dataset-root\", default=default_dataset_dir)\n",
    "parser.add_argument(\"--log-dir\", default=Path(\"logs\"), type=Path)\n",
    "parser.add_argument(\"--learning-rate\", default=1e-2, type=float, help=\"Learning rate\")\n",
    "parser.add_argument(\n",
    "    \"--batch-size\",\n",
    "    default=128,\n",
    "    type=int,\n",
    "    help=\"Number of images within each mini-batch\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--epochs\",\n",
    "    default=20,\n",
    "    type=int,\n",
    "    help=\"Number of epochs (passes through the entire dataset) to train for\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--val-frequency\",\n",
    "    default=2,\n",
    "    type=int,\n",
    "    help=\"How frequently to test the model on the validation set in number of epochs\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--log-frequency\",\n",
    "    default=10,\n",
    "    type=int,\n",
    "    help=\"How frequently to save logs to tensorboard in number of steps\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--print-frequency\",\n",
    "    default=10,\n",
    "    type=int,\n",
    "    help=\"How frequently to print progress to the command line in number of steps\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-j\",\n",
    "    \"--worker-count\",\n",
    "    default=cpu_count(),\n",
    "    type=int,\n",
    "    help=\"Number of worker processes used to load data.\",\n",
    ")\n",
    "parser.add_argument(\"--sgd-momentum\", default=0, type=float)\n",
    "parser.add_argument(\"--data-aug-hflip\", action=\"store_true\")\n",
    "\n",
    "\n",
    "\n",
    "class ImageShape(NamedTuple):\n",
    "    height: int\n",
    "    width: int\n",
    "    channels: int\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    \n",
    "    args.dataset_root.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "\n",
    "    if args.data_aug_hflip:\n",
    "        train_dataset = torchvision.datasets.CIFAR10(args.dataset_root, train=True, download=True, transform=transforms.Compose([transforms.RandomHorizontalFlip(), transforms.ToTensor()]))\n",
    "    else:\n",
    "        train_dataset = torchvision.datasets.CIFAR10(args.dataset_root, train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        args.dataset_root, train=False, download=False, transform=transforms.ToTensor()\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,\n",
    "        batch_size=args.batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=args.worker_count,\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.worker_count,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    model = CNN(height=32, width=32, channels=3, class_count=10)\n",
    "\n",
    "    ## TASK 8: Redefine the criterion to be softmax cross entropy\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    ## TASK 11: Define the optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=args.learning_rate, momentum=args.sgd_momentum)\n",
    "\n",
    "    log_dir = get_summary_writer_log_dir(args)\n",
    "    print(f\"Writing logs to {log_dir}\")\n",
    "    summary_writer = SummaryWriter(\n",
    "            str(log_dir),\n",
    "            flush_secs=5\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model, train_loader, test_loader, criterion, optimizer, summary_writer, DEVICE\n",
    "    )\n",
    "\n",
    "    trainer.train(\n",
    "        args.epochs,\n",
    "        args.val_frequency,\n",
    "        print_frequency=args.print_frequency,\n",
    "        log_frequency=args.log_frequency,\n",
    "    )\n",
    "\n",
    "    summary_writer.close()\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, height: int, width: int, channels: int, class_count: int):\n",
    "        super().__init__()\n",
    "        self.input_shape = ImageShape(height=height, width=width, channels=channels)\n",
    "        self.class_count = class_count\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=self.input_shape.channels,\n",
    "            out_channels=32,\n",
    "            kernel_size=(5, 5),\n",
    "            padding=(2, 2),\n",
    "        )\n",
    "        self.initialise_layer(self.conv1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "        # BatchNorm layer after first convolutional layer\n",
    "\n",
    "        self.batchnorm1 = nn.BatchNorm2d(self.conv1.out_channels)\n",
    "\n",
    "\n",
    "        ## TASK 2-1: Define the second convolutional layer and initialise its parameters\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32,\n",
    "            out_channels=64,\n",
    "            kernel_size=(5, 5),\n",
    "            padding=(2, 2)\n",
    "        )\n",
    "\n",
    "        self.initialise_layer(self.conv2)\n",
    "\n",
    "        \n",
    "    \n",
    "        ## TASK 3-1: Define the second pooling layer\n",
    "\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "\n",
    "\n",
    "        # BatchNorm layer after second convolutional layer\n",
    "\n",
    "        self.batchnorm2 = nn.BatchNorm2d(num_features=self.conv2.out_channels)\n",
    "\n",
    "        \n",
    "        ## TASK 5-1: Define the first FC layer and initialise its parameters\n",
    "\n",
    "        self.fc1 = nn.Linear(4096, 1024)\n",
    "        self.initialise_layer(self.fc1)\n",
    "\n",
    "        # BatchNorm layer after first FC layer\n",
    "        # import pdb; pdb.set_trace()\n",
    "        self.batchnorm3 = nn.BatchNorm1d(num_features=self.fc1.out_features)\n",
    "\n",
    "        \n",
    "        ## TASK 6-1: Define the last FC layer and initialise its parameters10\n",
    "\n",
    "        self.fc2 = nn.Linear(1024, 10)\n",
    "        self.initialise_layer(self.fc2)\n",
    "\n",
    "        # BatchNorm layer after second FC layer\n",
    "\n",
    "        self.batchnorm4 = nn.BatchNorm1d(num_features=self.fc2.out_features)\n",
    "\n",
    "\n",
    "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.conv1(images))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.batchnorm1(x)\n",
    "        ## TASK 2-2: Pass x through the second convolutional layer\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        \n",
    "        ## TASK 3-2: Pass x through the second pooling layer\n",
    "        \n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.batchnorm2(x)\n",
    "\n",
    "        ## TASK 4: Flatten the output of the pooling layer so it is of shape\n",
    "        ##         (batch_size, 4096)\n",
    "        \n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        \n",
    "        \n",
    "        ## TASK 5-2: Pass x through the first fully connected layer\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = self.batchnorm3(x)\n",
    "        \n",
    "        ## TASK 6-2: Pass x through the last fully connected layer\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        x = self.batchnorm4(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def initialise_layer(layer):\n",
    "        if hasattr(layer, \"bias\"):\n",
    "            nn.init.zeros_(layer.bias)\n",
    "        if hasattr(layer, \"weight\"):\n",
    "            nn.init.kaiming_normal_(layer.weight)\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        criterion: nn.Module,\n",
    "        optimizer: Optimizer,\n",
    "        summary_writer: SummaryWriter,\n",
    "        device: torch.device,\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.summary_writer = summary_writer\n",
    "        self.step = 0\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        epochs: int,\n",
    "        val_frequency: int,\n",
    "        print_frequency: int = 20,\n",
    "        log_frequency: int = 5,\n",
    "        start_epoch: int = 0\n",
    "    ):\n",
    "        self.model.train()\n",
    "        for epoch in range(start_epoch, epochs):\n",
    "            self.model.train()\n",
    "            data_load_start_time = time.time()\n",
    "            for batch, labels in self.train_loader:\n",
    "                batch = batch.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                data_load_end_time = time.time()\n",
    "\n",
    "\n",
    "                ## TASK 1: Compute the forward pass of the model, print the output shape\n",
    "                ##         and quit the program\n",
    "                logits = self.model.forward(batch)\n",
    "                # print(output.shape)\n",
    "                # import sys; sys.exit(1)\n",
    "\n",
    "                ## TASK 7: Rename `output` to `logits`, remove the output shape printing\n",
    "                ##         and get rid of the `import sys; sys.exit(1)`\n",
    "\n",
    "                # Task 7 done above\n",
    "\n",
    "                ## TASK 9: Compute the loss using self.criterion and\n",
    "                ##         store it in a variable called `loss`\n",
    "\n",
    "                loss = self.criterion(logits, labels)\n",
    "\n",
    "                ## TASK 10: Compute the backward pass\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                ## TASK 12: Step the optimizer and then zero out the gradient buffers.\n",
    "                \n",
    "                self.optimizer.step()\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    preds = logits.argmax(-1)\n",
    "                    accuracy = compute_accuracy(labels, preds)\n",
    "\n",
    "                data_load_time = data_load_end_time - data_load_start_time\n",
    "                step_time = time.time() - data_load_end_time\n",
    "                if ((self.step + 1) % log_frequency) == 0:\n",
    "                    self.log_metrics(epoch, accuracy, loss, data_load_time, step_time)\n",
    "                if ((self.step + 1) % print_frequency) == 0:\n",
    "                    self.print_metrics(epoch, accuracy, loss, data_load_time, step_time)\n",
    "\n",
    "                self.step += 1\n",
    "                data_load_start_time = time.time()\n",
    "\n",
    "            self.summary_writer.add_scalar(\"epoch\", epoch, self.step)\n",
    "            if ((epoch + 1) % val_frequency) == 0:\n",
    "                self.validate()\n",
    "                # self.validate() will put the model in validation mode,\n",
    "                # so we have to switch back to train mode afterwards\n",
    "                self.model.train()\n",
    "\n",
    "    def print_metrics(self, epoch, accuracy, loss, data_load_time, step_time):\n",
    "        epoch_step = self.step % len(self.train_loader)\n",
    "        print(\n",
    "                f\"epoch: [{epoch}], \"\n",
    "                f\"step: [{epoch_step}/{len(self.train_loader)}], \"\n",
    "                f\"batch loss: {loss:.5f}, \"\n",
    "                f\"batch accuracy: {accuracy * 100:2.2f}, \"\n",
    "                f\"data load time: \"\n",
    "                f\"{data_load_time:.5f}, \"\n",
    "                f\"step time: {step_time:.5f}\"\n",
    "        )\n",
    "\n",
    "    def log_metrics(self, epoch, accuracy, loss, data_load_time, step_time):\n",
    "        self.summary_writer.add_scalar(\"epoch\", epoch, self.step)\n",
    "        self.summary_writer.add_scalars(\n",
    "                \"accuracy\",\n",
    "                {\"train\": accuracy},\n",
    "                self.step\n",
    "        )\n",
    "        self.summary_writer.add_scalars(\n",
    "                \"loss\",\n",
    "                {\"train\": float(loss.item())},\n",
    "                self.step\n",
    "        )\n",
    "        self.summary_writer.add_scalar(\n",
    "                \"time/data\", data_load_time, self.step\n",
    "        )\n",
    "        self.summary_writer.add_scalar(\n",
    "                \"time/data\", step_time, self.step\n",
    "        )\n",
    "\n",
    "    def validate(self):\n",
    "        results = {\"preds\": [], \"labels\": []}\n",
    "        total_loss = 0\n",
    "        self.model.eval()\n",
    "\n",
    "        # No need to track gradients for validation, we're not optimizing.\n",
    "        with torch.no_grad():\n",
    "            for batch, labels in self.val_loader:\n",
    "                batch = batch.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                logits = self.model(batch)\n",
    "                loss = self.criterion(logits, labels)\n",
    "                total_loss += loss.item()\n",
    "                preds = logits.argmax(dim=-1).cpu().numpy()\n",
    "                results[\"preds\"].extend(list(preds))\n",
    "                results[\"labels\"].extend(list(labels.cpu().numpy()))\n",
    "\n",
    "        accuracy = compute_accuracy(\n",
    "            np.array(results[\"labels\"]), np.array(results[\"preds\"])\n",
    "        )\n",
    "        average_loss = total_loss / len(self.val_loader)\n",
    "\n",
    "        self.summary_writer.add_scalars(\n",
    "                \"accuracy\",\n",
    "                {\"test\": accuracy},\n",
    "                self.step\n",
    "        )\n",
    "        self.summary_writer.add_scalars(\n",
    "                \"loss\",\n",
    "                {\"test\": average_loss},\n",
    "                self.step\n",
    "        )\n",
    "        print(f\"validation loss: {average_loss:.5f}, accuracy: {accuracy * 100:2.2f}\")\n",
    "\n",
    "\n",
    "def compute_accuracy(\n",
    "    labels: Union[torch.Tensor, np.ndarray], preds: Union[torch.Tensor, np.ndarray]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        labels: ``(batch_size, class_count)`` tensor or array containing example labels\n",
    "        preds: ``(batch_size, class_count)`` tensor or array containing model prediction\n",
    "    \"\"\"\n",
    "    assert len(labels) == len(preds)\n",
    "    return float((labels == preds).sum()) / len(labels)\n",
    "\n",
    "\n",
    "def get_summary_writer_log_dir(args: argparse.Namespace) -> str:\n",
    "    \"\"\"Get a unique directory that hasn't been logged to before for use with a TB\n",
    "    SummaryWriter.\n",
    "\n",
    "    Args:\n",
    "        args: CLI Arguments\n",
    "\n",
    "    Returns:\n",
    "        Subdirectory of log_dir with unique subdirectory name to prevent multiple runs\n",
    "        from getting logged to the same TB log directory (which you can't easily\n",
    "        untangle in TB).\n",
    "    \"\"\"\n",
    "    tb_log_dir_prefix = (\n",
    "      f\"CNN_bn_\"\n",
    "      f\"bs={args.batch_size}_\"\n",
    "      f\"lr={args.learning_rate}_\"\n",
    "      f\"momentum=0.9_\" +\n",
    "      (\"hflip_\" if args.data_aug_hflip else \"\") +\n",
    "      f\"run_\"\n",
    "  )\n",
    "    i = 0\n",
    "    while i < 1000:\n",
    "        tb_log_dir = args.log_dir / (tb_log_dir_prefix + str(i))\n",
    "        if not tb_log_dir.exists():\n",
    "            return str(tb_log_dir)\n",
    "        i += 1\n",
    "    return str(tb_log_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(parser.parse_args())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
